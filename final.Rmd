---
title: "Enos 2016: Replication and Extension"
output:
 pdf_document:
  md_extension: +raw_tex
bibliography: citations.bib
---

```{r setup, include=FALSE}

# Replication of "What the Demolition of Public Housing Teaches Us about the Impact of Racial Threat on Political Behavior"
# Published in American Journal of Political Science (2015)
# Original author: Ryan D. Enos

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(extrafont)
library(stargazer)
library(apsrtable)
library(Zelig)
library(simpleboot)
library(boot)
library(viridis)
```

\begin{flushright}
Gabe Walker

April 10, 2019
\end{flushright}

# Abstract

## What the Demolition of Public Housing Teaches Us about the Impact of Racial Threat on Political Behavior
I replicate the figures in @enos, "[What the Demolition of Public Housing Teaches Us about the Impact of Racial Threat on Political Behavior](https://scholar.harvard.edu/files/renos/files/enoschicago.pdf)," the data for which can be found on the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26612) [@enos_data].[^1]

The figures appear exactly as in the original. They show the "treatment" effect of the quasi-experimental demolishing of housing projects in Chicago on the voting behavior of the nearby white population. Overall, the figures underscore the paper's argument that white individuals living closer to the projects, or those living in an area where most of the neighborhood's black population was living in a demolished project, voted less after the projects came down. Enos argues that this is empirical evidence for the "racial threat" hypothesis.

The extension attempts to depict the key finding of the paper in a more visually compelling way. Using the mean treatment effects for four different categories of individuals, I illustrate that the white treatment groups show decreased voting turnout at closer distances than black treatment groups.

[^1]: Note that to knit this document, you must change your working directory in line 785 of the code to read in the large CSV file of main results.

\pagebreak

```{r data, echo=FALSE, message=FALSE, error=FALSE}

# First download the replication file provided on the Harvard Dataverse:
# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/26612

# Load data.

wtreat <- read.csv("dataverse_files/white.treat.effect.mean.boot.csv")
wtreat.lower <- read.csv("dataverse_files/white.treat.effect.conf.boot.lower.csv")
wtreat.upper <- read.csv("dataverse_files/white.treat.effect.conf.boot.upper.csv")
Nwtreat <- read.csv("dataverse_files/white.treat.N.csv")
btreat <- read.csv("dataverse_files/black.treat.effect.mean.boot.csv")
btreat.lower <- read.csv("dataverse_files/black.treat.effect.conf.boot.lower.csv")
btreat.upper <- read.csv("dataverse_files/black.treat.effect.conf.boot.upper.csv")
Nbtreat <- read.csv("dataverse_files/black.treat.N.csv")

# Letters for marking graphs. One is not used.

use.letters <- c("a", "b", "c", "d", "e", "f", "skip", "g", "h")

# Cycle through each line of data, each of which are groups defined by diferent namepcts.
# Turning into matrices helps below with segment function

for (i in 1:nrow(wtreat)) {
  use.wtreat <- as.matrix(wtreat[i, ])
  use.wlower <- as.matrix(wtreat.lower[i, ])
  use.wupper <- as.matrix(wtreat.upper[i, ])
  use.Nwtreat <- as.matrix(Nwtreat[i, ])

  use.btreat <- as.matrix(btreat[i, ])
  use.blower <- as.matrix(btreat.lower[i, ])
  use.bupper <- as.matrix(btreat.upper[i, ])
  use.Nbtreat <- as.matrix(Nbtreat[i, ])
}
```

```{r graphics, echo=FALSE, message=FALSE, error=FALSE}

# Master parameters for graphics

ylims <- c(-.35, .1)
ylims.2 <- c(-.45, .1)
xlims <- c(.5, 11)
dists <- seq(from = 1000, to = 100, by = -100) ### DELETE THIS LATER
xs <- seq(1:length(dists))
ys <- seq(from = -.35, to = .1, by = .05)
ys.lab <- c("-0.35", "-0.30", "-0.25", "-0.20", "-0.15", "-0.10", "-0.05", "0.00", "0.05", "0.10")
ys.2 <- seq(from = -.45, to = .1, by = .05)
ys.lab.2 <- c("-0.45", "-0.40", "-0.35", "-0.30", "-0.25", "-0.20", "-0.15", "-0.10", "-0.05", "0.00", "0.05", "0.10")

offsets <- .15
text.offsets <- .025
cex.axis <- .9
cex.N <- .7
top.text.adj <- c(1.3, 1.3) ## Offsets on labels to reduce crowding
bottom.text.adj <- c(-.15, -.85)
point.size <- 2
line.offset <- .0175
```

# Figure 1 Treatment Effects

***

```{r fig1, echo=FALSE, message=FALSE, error=FALSE}

# Set the graphical parameters.

par(las = 1)
par(mar = c(5.1, 4.1, .5, .5))
plot(xs, use.wtreat,
  ylim = ylims,
  xlim = xlims,
  type = "n",
  ylab = "Treatment Effect",
  xlab = "Treated Group Distance from Projects",
  xaxt = "n",
  yaxt = "n.csv"
)
abline(h = 0, lty = 2)

# Draw the error bars first.

segments(
  x0 = xs[1:2] + offsets, x1 = xs[1:2] + offsets, ## only do it for low N blacks because otherwise lines look funny
  y0 = use.btreat[, 1:2], y1 = use.blower[, 1:2]
)
segments(
  x0 = xs[1:2] + offsets, x1 = xs[1:2] + offsets,
  y0 = use.btreat[, 1:2] + line.offset, y1 = use.bupper[, 1:2]
)
## now the others
segments(
  x0 = xs[3:10] + offsets, x1 = xs[3:10] + offsets,
  y0 = use.blower[, 3:10], y1 = use.bupper[, 3:10]
)

segments(
  x0 = xs - offsets, x1 = xs - offsets, ## bottomlines
  y0 = use.wtreat - line.offset, y1 = use.wlower
)
segments(
  x0 = xs - offsets, x1 = xs - offsets, ## toplines
  y0 = use.wtreat, y1 = use.wupper
)

# Lay down the points and add text equal to the sample size.

points(xs - offsets, use.wtreat,
  cex = point.size,
  pch = 21,
  bg = "white",
  col = "black"
)
text(xs - offsets, use.wtreat,
  paste("(", use.Nwtreat, ")", sep = ""),
  cex = cex.N,
  # adj = top.text.adj
  pos = 1
)

points(xs + offsets, use.btreat,
  pch = 16,
  cex = point.size
)
text(xs + offsets, use.btreat,
  paste("(", use.Nbtreat, ")", sep = ""),
  cex = cex.N,
  # adj = bottom.text.adj
  pos = 3
)

# Label the axes.

axis(
  side = 1,
  at = xs,
  label = seq(100, 1000, 100),
  cex.axis = cex.axis
)
axis(
  side = 2,
  at = ys,
  label = ys.lab,
  cex.axis = cex.axis
)
```

***

*Note*: Difference-in-differences results for treatment groups defined by increasing distance from
the demolished projects. Differences are for the mean turnout in 2004 minus the mean turnout
in 2000 for the treatment group minus the same difference for the control group. White circles
represent the mean effect on white voters; black circles represent the mean effect on black voters.
The N in each treatment group is in parentheses next to the mean effect. Vertical lines represent
the 95% confidence intervals generated by bootstrapped standard errors of the difference between
treatment and control.

\pagebreak

# Figure 2 Treatment Effects Using Matched White Voters Near Nondemolished Projects for Control Group

***

```{r fig2, echo=FALSE, message=FALSE, error=FALSE}

# Figure 2.

# Read in the relevant files.

treat <- read.csv("dataverse_files/white.match.nondemolished.csv")
diffs <- read.csv("dataverse_files/white.match.nondemolished.diffs.csv")

# Set the points to triangles.

pchs <- 17

# Define axes for different graphs.

use.ylims <- ylims
use.ys.lab <- ys.lab
use.ys <- ys

# Go through pairs for each pair of dataframe to find the confidence intervals.

use.treat <- treat$coefficient
clower <- use.treat - (1.96 * treat$stdev)
cupper <- use.treat + (1.96 * treat$stdev)
use.N.treat <- treat$N.treatment + treat$N.control

# Set the graphical parameters.

par(las = 1)
par(mar = c(5.1, 4.1, .5, .5))
plot(xs, use.treat,
  ylim = use.ylims,
  xlim = xlims,
  type = "n",
  ylab = "Treatment Effect",
  xlab = "Treated Group Distance from Projects",
  xaxt = "n",
  yaxt = "n"
)

# Make a horizontal dashed line.

abline(h = 0, lty = 2)

# Draw the confidence intervals.

segments(
  x0 = xs, x1 = xs,
  y0 = use.treat + line.offset, y1 = cupper
)
segments(
  x0 = xs, x1 = xs,
  y0 = use.treat, y1 = clower
)

# Print black triangles.

points(xs, use.treat,
  pch = pchs,
  cex = point.size,
  bg = "white",
  col = "black"
)

# Print the N size of each group.

text(xs, use.treat,
  paste("(", use.N.treat, ")", sep = ""),
  cex = cex.N,
  pos = 3
)

# Axis labels.

axis(
  side = 1,
  at = xs,
  label = seq(100, 1000, 100),
  cex.axis = cex.axis
)
axis(
  side = 2,
  at = use.ys,
  label = use.ys.lab,
  cex.axis = cex.axis
)
```

***

*Note*: Coefficients on treatment as defined by increasing distance from the demolished projects
from OLS regressions on change in turnout from 2000 to 2004 (triangles). N for the regression
using matched groups is next to the point representing the coefficient. The treatment group is
matched to a control group of white voters living near projects that were not demolished, using
nearest neighbor matching. Regressions include variables used in matching as controls. Vertical
lines represent the 95% confidence intervals generated by bootstrapped standard errors on the
treatment coefficient.

\pagebreak

# Figure 3 Treatment Effects Using Matched Black Control Group and Controlling for Homeownership

***

```{r fig3, echo=FALSE, message=FALSE, error=FALSE}

# This is the same exact code as in the fig2 chunk above, just with different data.

treat <- read.csv("dataverse_files/white.match.black.property.csv")
diffs <- read.csv("dataverse_files/white.match.black.diffs.property.csv")

# Set the points to triangles.

pchs <- 17

# Define axes for different graphs.

use.ylims <- ylims
use.ys.lab <- ys.lab
use.ys <- ys

# Go through pairs for each pair of dataframe to find the confidence intervals.

use.treat <- treat$coefficient
clower <- use.treat - (1.96 * treat$stdev)
cupper <- use.treat + (1.96 * treat$stdev)
use.N.treat <- treat$N.treatment + treat$N.control

# Set the graphical parameters.

par(las = 1)
par(mar = c(5.1, 4.1, .5, .5))
plot(xs, use.treat,
  ylim = use.ylims,
  xlim = xlims,
  type = "n",
  ylab = "Treatment Effect",
  xlab = "Treated Group Distance from Projects",
  xaxt = "n",
  yaxt = "n"
)

# Make a horizontal dashed line.

abline(h = 0, lty = 2)

# Draw the confidence intervals.

segments(
  x0 = xs, x1 = xs,
  y0 = use.treat + line.offset, y1 = cupper
)
segments(
  x0 = xs, x1 = xs,
  y0 = use.treat, y1 = clower
)

# Print black triangles.

points(xs, use.treat,
  pch = pchs,
  cex = point.size,
  bg = "white",
  col = "black"
)

# Print the N size of each group.

text(xs, use.treat,
  paste("(", use.N.treat, ")", sep = ""),
  cex = cex.N,
  pos = 3
)

# Axis labels.

axis(
  side = 1,
  at = xs,
  label = seq(100, 1000, 100),
  cex.axis = cex.axis
)
axis(
  side = 2,
  at = use.ys,
  label = use.ys.lab,
  cex.axis = cex.axis
)
```

***

*Note*: Coefficients on treatment as defined by increasing distance from the demolished projects
from OLS regressions on change in turnout from 2004 to 2000 (triangles). N for the regression
using matched groups is next to the point representing the coefficient. The white treatment group
is matched to a black control group of the same N using nearest neighbor matching and including
variables on homeownership and home value. Regressions include variables used in matching as
controls. Vertical lines represent the 95% confidence intervals generated by bootstrapped standard
errors on the treatment coefficient.

\pagebreak

# Figure 4 Effects of Distance and Size of Projects

***

```{r fig4a, echo=FALSE, message=FALSE, error=FALSE, fig.dim=c(3,3), fig.align='center'}

# Figure 4a.

# Read in the two data files.

distdat <- read.csv("dataverse_files/predicted.results.distance.vary.context.csv")
areadat <- read.csv("dataverse_files/predicted.results.area.vary.context.csv")

# New ylims for these graphs

ylims.predict <- c(.6, .75)

# Cycle through both datasets.

datas <- list(distdat, areadat)

# Graphical parameters.

xs <- list(seq(from = 10, to = 2000, by = 10), seq(from = 45000, to = 1004000, by = 4800) / 1000)
use.letters <- c("a", "b")
xlabs <- c("Distance from Project", "Percent of Local Black Population in Demolished Project")
ylabs <- c(expression(Pr(vote[2004])), "")
vlines <- list(seq(from = 0, to = 2000, by = 200), seq(from = 0, to = 1000, by = 100))
axis.labs <- list(
  as.character(seq(from = 0, to = 2000, by = 200)),
  as.character(c("0", "10%", "20%", "30%", "40%", "50%", "60%", "70%", "80%", "90%", "100%"))
)

# Draw one plot at a time.

for (i in 1) {
  colnames(datas[[i]]) <- c("mean", "sd", "50%", "2.5%", "97.5%")

  # Add a pty = "s" here so that the outputs are square, as in the paper.

  par(las = 1, pty = "s")
  par(mar = c(5.1, 4.1, .5, .5))
  plot(xs[[i]], datas[[i]][, "mean"],
    type = "l",
    xlab = xlabs[i],
    ylab = ylabs[i],
    ylim = ylims.predict,
    xaxt = "n",
    cex.axis = cex.axis,
    lwd = 4
  )

  # Put horizontal and vertical lines on plots

  abline(
    h = seq(from = min(ylims.predict), to = max(ylims.predict), by = .025),
    lty = 2,
    col = "gray",
    lwd = 1
  )
  abline(
    v = vlines[[i]],
    lty = 2,
    col = "gray",
    lwd = 1
  )
  lines(xs[[i]], datas[[i]][, "2.5%"],
    lty = 3,
    lwd = 2.5
  )
  lines(xs[[i]], datas[[i]][, "97.5%"],
    lty = 3,
    lwd = 2.5
  )
  axis(
    side = 1,
    at = vlines[[i]],
    labels = axis.labs[[i]],
    cex.axis = cex.axis
  )
}
```

```{r fig4b, echo=FALSE, message=FALSE, error=FALSE, fig.dim=c(5,3), fig.align='center'}

# Figure 4b.

# Read in the two data files.

distdat <- read.csv("dataverse_files/predicted.results.distance.vary.context.csv")
areadat <- read.csv("dataverse_files/predicted.results.area.vary.context.csv")

# New ylims for these graphs

ylims.predict <- c(.6, .75)

# Cycle through both datasets.

datas <- list(distdat, areadat)

# Graphical parameters.

xs <- list(seq(from = 10, to = 2000, by = 10), seq(from = 45000, to = 1004000, by = 4800) / 1000)
use.letters <- c("a", "b")
xlabs <- c("Distance from Project", "Percent of Local Black Population in Demolished Project")
ylabs <- c(expression(Pr(vote[2004])), "")
vlines <- list(seq(from = 0, to = 2000, by = 200), seq(from = 0, to = 1000, by = 100))
axis.labs <- list(
  as.character(seq(from = 0, to = 2000, by = 200)),
  as.character(c("0", "10%", "20%", "30%", "40%", "50%", "60%", "70%", "80%", "90%", "100%"))
)

# Draw one plot at a time.

for (i in 2) {
  colnames(datas[[i]]) <- c("mean", "sd", "50%", "2.5%", "97.5%")

  # Add a pty = "s" here so that the outputs are square, as in the paper.

  par(las = 1, pty = "s")
  par(mar = c(5.1, 4.1, .5, .5))
  plot(xs[[i]], datas[[i]][, "mean"],
    type = "l",
    xlab = xlabs[i],
    ylab = ylabs[i],
    ylim = ylims.predict,
    xaxt = "n",
    cex.axis = cex.axis,
    lwd = 4
  )

  # Put horizontal and vertical lines on plots

  abline(
    h = seq(from = min(ylims.predict), to = max(ylims.predict), by = .025),
    lty = 2,
    col = "gray",
    lwd = 1
  )
  abline(
    v = vlines[[i]],
    lty = 2,
    col = "gray",
    lwd = 1
  )
  lines(xs[[i]], datas[[i]][, "2.5%"],
    lty = 3,
    lwd = 2.5
  )
  lines(xs[[i]], datas[[i]][, "97.5%"],
    lty = 3,
    lwd = 2.5
  )
  axis(
    side = 1,
    at = vlines[[i]],
    labels = axis.labs[[i]],
    cex.axis = cex.axis
  )
}
```


***

*Note*: Predicted effects generated from $vote_{2004} = \beta_0 + \beta_1(log(distance)) + \beta_2(log(localpercent)) + vote_{2000}$, with white voters.
Figure 4(a) is the predicted probability that a person who voted in 2000 will vote in 2004 with increasing distance, while holding
size at its mean. Figure 4(b) is the predicted probability that a person who voted in 2000 will vote in 2004, with increasing outgroup
population size, with *distance* = 100. Dotted lines represent 95% confidence intervals generated by bootstrapped standard errors.

\pagebreak

# Figure 5

***

```{r fig5, echo=FALSE, message=FALSE, error=FALSE, fig.dim=c(3,3)}

pres.elections <- c("dole_pct_ei", "bush2000_pct_ei", "bush2004_pct_ei", "mccain_pct_ei")
obama.elections <- c("obama_sen_primary_pct_ei", "keyes_pct_ei", "obama_pres_primary_pct_ei")

dists <- read.csv("dataverse_files/distance.vote.differences.csv")
demos <- read.csv("dataverse_files/demolished.vote.differences.csv")

graphs <- c("5a", "5b")

for (i in graphs) {
  if (i == "5a") {
    dat <- dists
  }
  else {
    dat <- demos
  }

  if (i %in% c("5a", "5b")) {
    xlims <- c(.75, 4.25)
    ylims <- c(-.1, .2)
  }

  ## recode Keyes to Obama general for presentation purposes

  dat[dat$election == "keyes_pct_ei", "x.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "x.mean"]
  dat[dat$election == "keyes_pct_ei", "y.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "y.mean"]
  dat[dat$election == "keyes_pct_ei", "diff"] <- dat[dat$election == "keyes_pct_ei", "y.mean"] - dat[dat$election == "keyes_pct_ei", "x.mean"]

  par(las = 1)
  par(mar = c(5.1, 4.1, .5, 1.5))
  plot(seq(1:4),
    rep(1, 4),
    ylim = ylims,
    xlim = xlims,
    type = "n",
    xaxt = "n",
    yaxt = "n",
    xlab = "Election",
    ylab = ifelse(i == "5b", "", "Treatment Effect")
  )
  abline(h = 0, lty = 2)

  if (i %in% c("5a", "5b")) {
    segments(
      x0 = seq(1:4) - offsets,
      x1 = seq(1:4) - offsets,
      y0 = dat[dat$group == "white" & dat$election %in% pres.elections, "diff"] - (1.96 * dat[dat$group == "white" & dat$election %in% pres.elections, "sd"]),
      y1 = dat[dat$group == "white" & dat$election %in% pres.elections, "diff"] + (1.96 * dat[dat$group == "white" & dat$election %in% pres.elections, "sd"])
    )
    points(seq(1:4) - offsets,
      dat[dat$group == "white" & dat$election %in% pres.elections, "diff"],
      pch = 21,
      bg = "white",
      col = "black",
      cex = 2
    )
    segments(
      x0 = seq(1:4) + offsets,
      x1 = seq(1:4) + offsets,
      y0 = dat[dat$group == "black" & dat$election %in% pres.elections, "diff"] - (1.96 * dat[dat$group == "black" & dat$election %in% pres.elections, "sd"]),
      y1 = dat[dat$group == "black" & dat$election %in% pres.elections, "diff"] + (1.96 * dat[dat$group == "black" & dat$election %in% pres.elections, "sd"])
    )
    points(seq(1:4) + offsets,
      dat[dat$group == "black" & dat$election %in% pres.elections, "diff"],
      pch = 16,
      cex = 2
    )
    axis(
      side = 1, at = seq(1:4),
      c("1996", "2000", "2004", "2008"),
      tick = F,
      cex.axis = cex.axis
    )
  }
  axis(
    side = 2,
    at = seq(from = -.1, to = .3, by = .05),
    label = c("-0.10", "-0.05", "0.00", "0.05", "0.10", "0.15", "0.20", "0.25", "0.30"),
    cex.axis = cex.axis
  )
}
```

***

*Note*: Figure 5(a) shows differences inweighted mean Republican vote for precinctswith *d* $\geq$ 1,000 and matched precinctswith *d* > 1,000 for white voters (white circles) and black voters (black circles). Figure 5(*b*) shows differences in weighted mean Republican vote for white voters and black voters matched with precincts with d $\leq$ 1,000 from nondemolished projects.

\pagebreak

# Figure 6

***

```{r fig6, echo=FALSE, message=FALSE, error=FALSE}

pres.elections <- c("dole_pct_ei", "bush2000_pct_ei", "bush2004_pct_ei", "mccain_pct_ei")
obama.elections <- c("obama_sen_primary_pct_ei", "keyes_pct_ei", "obama_pres_primary_pct_ei")

dists <- read.csv("dataverse_files/distance.vote.differences.csv")
demos <- read.csv("dataverse_files/demolished.vote.differences.csv")

graphs <- c("6")

for (i in graphs) {
  if (i == "5a") {
    dat <- dists
  }
  else {
    dat <- demos
  }

  if (i %in% c("5a", "5b")) {
    xlims <- c(.75, 4.25)
    ylims <- c(-.1, .2)
  }
  else {
    xlims <- c(.75, 3.25)
    ylims <- c(-.1, .25)
  }

  ## recode Keyes to Obama general for presentation purposes

  dat[dat$election == "keyes_pct_ei", "x.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "x.mean"]
  dat[dat$election == "keyes_pct_ei", "y.mean"] <- 1 - dat[dat$election == "keyes_pct_ei", "y.mean"]
  dat[dat$election == "keyes_pct_ei", "diff"] <- dat[dat$election == "keyes_pct_ei", "y.mean"] - dat[dat$election == "keyes_pct_ei", "x.mean"]

  par(las = 1)
  par(mar = c(5.1, 4.1, .5, 1.5))
  plot(seq(1:4),
    rep(1, 4),
    ylim = ylims,
    xlim = xlims,
    type = "n",
    xaxt = "n",
    yaxt = "n",
    xlab = "Election",
    ylab = ifelse(i == "5b", "", "Treatment Effect")
  )
  abline(h = 0, lty = 2)

  segments(
    x0 = seq(1:3) - offsets,
    x1 = seq(1:3) - offsets,
    y0 = dat[dat$group == "white" & dat$election %in% obama.elections, "diff"] - (1.96 * dat[dat$group == "white" & dat$election %in% obama.elections, "sd"]),
    y1 = dat[dat$group == "white" & dat$election %in% obama.elections, "diff"] + (1.96 * dat[dat$group == "white" & dat$election %in% obama.elections, "sd"])
  )
  points(seq(1:3) - offsets,
    dat[dat$group == "white" & dat$election %in% obama.elections, "diff"],
    pch = 21,
    bg = "white",
    col = "black",
    cex = 2
  )
  segments(
    x0 = seq(1:3) + offsets,
    x1 = seq(1:3) + offsets,
    y0 = dat[dat$group == "black" & dat$election %in% obama.elections, "diff"] - (1.96 * dat[dat$group == "black" & dat$election %in% obama.elections, "sd"]),
    y1 = dat[dat$group == "black" & dat$election %in% obama.elections, "diff"] + (1.96 * dat[dat$group == "black" & dat$election %in% obama.elections, "sd"])
  )
  points(seq(1:3) + offsets,
    dat[dat$group == "black" & dat$election %in% obama.elections, "diff"],
    pch = 16,
    cex = 2
  )
  axis(
    side = 1, at = seq(1:3),
    c("2004 \n Senate Primary", "2004 \n Senate General", "2008 \n President Primary"),
    tick = F,
    cex.axis = cex.axis
  )
  axis(
    side = 2,
    at = seq(from = -.1, to = .3, by = .05),
    label = c("-0.10", "-0.05", "0.00", "0.05", "0.10", "0.15", "0.20", "0.25", "0.30"),
    cex.axis = cex.axis
  )
}
```

***

*Note*: Differences in weighted mean Obama vote for precincts with *d* $\leq$ 1,000 for demolished projects and matched precincts with *d* $\leq$ 1,000 for nondemolished projects for white voters (white circles) and black voters (black circles).

\pagebreak

```{r turnout, echo=FALSE, cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}

### Estimate the voter turnout from the replication file "turnout.r".

# Start time of the analysis.

ptm <- proc.time()

# Set the working directory to the desktop, where you should save data.turnout.csv.
# NOTE: this should be changed based on where the file is on your computer.

setwd("C:/Users/Gabriel/Desktop")

# Read in the large dataset and reset the working directory if need be.

data <- read.csv('data.turnout.csv')

# setwd("C:/Users/Gabriel/Dropbox/HKS/Courses/Spring 2019/Gov 1006 - Models/Replications/enos_2016")

# Change some variables to factors.

data$reg <- as.Date(data$reg)
data$p <- as.factor(data$p)
data$s <- as.factor(data$s)

# Distances used repeatedly in estimation below.

dists <- seq(from = 100, to = 1000, by = 100)

# Begin the dif in dif analysis and print a status report.

# cat('Begin basic difference-in-differences estimation. \n')

# Store some percentage values from 91 to 100 percent.

namepcts <- c(seq(from = .91, to = .96, by = .01),.975,.99,1)

# Matrices for storing results.

res.mat <- matrix(nrow=length(namepcts),ncol=length(dists))

white.treat.N <- res.mat
white.treat.effect.mean.boot <- res.mat
white.treat.effect.conf.boot.lower <- res.mat
white.treat.effect.conf.boot.upper <- res.mat

black.treat.N <- res.mat
black.treat.effect.mean.boot <- res.mat
black.treat.effect.conf.boot.lower <- res.mat
black.treat.effect.conf.boot.upper <- res.mat

# From replication: "Registration is Illionis is cutoff 27 days prior to election day, limit to these individuals." I.e., ignore individuals who are not registered? This is a poor way to segment data...

use.data <- data[data$reg <"2000-10-10" & is.na(data$reg) == F, ]

# Loop through definitions of white and distances and estimate at each combination.

for(j in 1:length(namepcts)){
  
# Define a treatment and control group for each name percent
  
	useW <- use.data[use.data$whitename >= namepcts[j],]
   useB <- use.data[use.data$blackname >= namepcts[j],]
  
    for(h in 1:length(dists)){
      	Wtreat <- useW[useW$demo.distance <= dists[h],]
      	Btreat <- useB[useB$demo.distance <= dists[h],]
      	Wcont <- useW[useW$demo.distance > dists[h],]
      	Bcont <- useB[useB$demo.distance > dists[h],]     		
      	white.treat.N[j,h] <- nrow(Wtreat)
      	black.treat.N[j,h] <- nrow(Btreat)
      	
# For white and black subjects, perform t test of differences of means with boostrapped standard errors.

# Leave out the bootstrapping for now to save time.

		if(white.treat.N[j,h] > 0){
			white.boot <- two.boot((Wtreat$vote2004-Wtreat$vote2000),(Wcont$vote2004-Wcont$vote2000), mean, R = 2, na.rm=T)
			white.treat.effect.mean.boot[j,h] <- white.boot$t0
			white.boot.ci <- boot.ci(white.boot, type = 'basic')
			white.treat.effect.conf.boot.lower[j,h] <- white.boot.ci$basic[4]
			white.treat.effect.conf.boot.upper[j,h] <- white.boot.ci$basic[5]
		      		}

		if(black.treat.N[j,h] > 0){
			black.boot <- two.boot((Btreat$vote2004-Btreat$vote2000),(Bcont$vote2004-Bcont$vote2000),mean, R = 2, na.rm=T)
			black.treat.effect.mean.boot[j,h] <- black.boot$t0
			black.boot.ci <- boot.ci(black.boot, type = 'basic')
			black.treat.effect.conf.boot.lower[j,h] <- black.boot.ci$basic[4]
			black.treat.effect.conf.boot.upper[j,h] <- black.boot.ci$basic[5]
			 }
		}
	}

# Print the elapsed time.

time.elapsed <- proc.time() - ptm

# cat(paste('Total elapsed time:', time.elapsed[3],'\n \n', sep = ' '))
```


```{r simulation, echo=FALSE, cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, results='hide'}

### Create predicted effects for distance and context changes.

# cat('Beginning turnout predictions with changing context and distance. \n')

# Define data for whites used in an earlier portion of the replication code (parallel trends test).

white.data <- data[as.Date(data$reg) < "2000-10-10" & is.na(data$reg) == F, ]

# Use subjects who "qualify by name percents" (not sure what this means yet.)

white.data <- white.data[white.data$whitename >= .975,]

usedata <- white.data

# Create distances and areas for estimating effects.

distances <- seq(from = 10, to = 2000, by = 10)

areas <- seq(from = 0, to = 1, length.out = length(distances))
			      
# Storing outcomes.

outmat.s <- matrix(ncol = 5, nrow = 0)
outmat.d <- matrix(ncol = 5, nrow = 0)

# Regression!

out.reg <- zelig(vote2004 ~ log(demo.distance) + log(context_black) + vote2000, data = usedata, model = 'ls', cite = F)

### Begin simulations across variable values.

for(i in seq(1:length(distances))){
	print(i)
	ptm <- proc.time()  # Start time.

	use.distance = distances[i]
	out.d.1 = setx(out.reg,
		vote2000 = 1,
		demo.distance = use.distance)

	out.d.sims = sim(out.reg,
		x = out.d.1)
	
	use.area = areas[i]	
	out.s.1 = setx(out.reg,
		vote2000 = 1,
		demo.distance = 100,
		context_black = use.area)

	out.s.sims = sim(out.reg,
		x = out.s.1)

	outstats.d = summary(out.d.sims)$stats[[1]]
	outstats.s = summary(out.s.sims)$stats[[1]]
	
	outmat.d = rbind(outmat.d,outstats.d)
	outmat.s = rbind(outmat.s,outstats.s)

	print(proc.time() - ptm)  # End time.
	}

# Store results for graphics and table.

predicted.results.distance.vary.context <- outmat.d
predicted.results.area.vary.context <- outmat.s

out.reg.predictions <- out.reg

time.elapsed <- proc.time() - ptm  # End time.
# cat(paste('total elapsed time:', time.elapsed[3],'\n \n',sep = ' '))
```

\pagebreak

# Table 1 Regression of Turnout on Distance and Population Size

```{r table1, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, results='asis', fig.align='center'}

### Table 1.

# Run a regular regression again so we can print the results.

out.reg2 <- lm(vote2004 ~ log(demo.distance) + log(context_black) + vote2000, data = usedata)

stargazer(out.reg2, header = FALSE, style = "ajps", title = "Regression of Turnout on Distance and Population Size")

# apsrtable(out.reg2,
# 	coef.names = c( 'Intercept','log(distance)','log(percent of local black population)','2000 turnout'),
# 	digits = 3
# 	)
```

*Notes*: OLS regression of 2004 voter turnout on listed variables for white voters. The local African American population is represented by the total number of African Americans within 1 kilometer of the project. The population of the housing projects is represented by the African American population of the census blocks containing the projects. The percent of the local African American population living in the demolished projects is these two quantities expressed as a proportion. Standard errors are listed in parentheses. All coefficients are significant at p < .0001.

\pagebreak

# Extension

This extension attempts to depict the key finding of the paper in a more visually compelling way. Using the mean treatment effects for four different categories of individuals, I illustrate that the white treatment groups show decreased voting turnout at closer distances than black treatment groups.

The four categories are:

* Black individuals (with 91 percent confidence, based on Enos's replication data)
* Black individuals (with 99 percent confidence)
* White individuals (91)
* White individuals (99)

Note that yellow in the charts below represents higher density.

```{r extension, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE}

### Extension: 

# Save the results of the mean treatment effect for blacks and whites
# at all distances at different confidence levels of accuracy for
# their race (91 and 99 percent, respectively).

b91 <- black.treat.effect.mean.boot[1, ]
b99 <- black.treat.effect.mean.boot[8, ]
w91 <- white.treat.effect.mean.boot[1, ]
w99 <- white.treat.effect.mean.boot[8, ]

# Create a tibble to store all the results in a tidy format.
# distance is distance from the demolition.
# race is predicted race.
# confidence is (probably) estimated confidence for racial category.
# effect is the predicted mean treatment effect.

results <- tibble(distance = rep(seq(100, 1000, by = 100), 4),
                  race = c(rep("black", 20), rep("white", 20)),
                  confidence = c(rep(91, 10), rep(99, 10), rep(91, 10), rep(99, 10)),
                  effect = c(b91, b99, w91, w99))

```

```{r p1, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.dim=c(5,5), fig.align='center'}
# Make contour plots for each of the four categories.

results %>% 
  filter(race == "black" & confidence == 91) %>% 
  ggplot(aes(distance, effect)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  scale_fill_viridis() +
  theme(panel.background = element_blank(), legend.position="none") +
  labs(x = "Distance (m)", y = "Treatment effect (p.p. voting change)", title = "Black residents decrease turnout at higher distances.", subtitle = "Black with 99 percent confidence.")
```

```{r p2, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.dim=c(5,5), fig.align='center'}
results %>% 
  filter(race == "black" & confidence == 99) %>% 
  ggplot(aes(distance, effect)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  scale_fill_viridis() +
  theme(panel.background = element_blank(), legend.position="none") +
  labs(x = "Distance (m)", y = "Treatment effect (p.p. voting change)", title = "Black residents decrease turnout at higher distances.", subtitle = "Black with 99 percent confidence.")
```

```{r p3, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.dim=c(5,5), fig.align='center'}
results %>% 
  filter(race == "white" & confidence == 91) %>% 
  ggplot(aes(distance, effect)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  scale_fill_viridis() +
  theme(panel.background = element_blank(), legend.position="none") +
  labs(x = "Distance (m)", y = "Treatment effect (p.p. voting change)", title = "White residents decrease turnout at closer distances.", subtitle = "White with 91 percent confidence.")
```

```{r p4, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.dim=c(5,5), fig.align='center'}
results %>% 
  filter(race == "white" & confidence == 99) %>% 
  ggplot(aes(distance, effect)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  scale_fill_viridis() +
  theme(panel.background = element_blank(), legend.position="none") +
  labs(x = "Distance (m)", y = "Treatment effect (p.p. voting change)", title = "White residents decrease turnout at closer distances.", subtitle = "White with 99 percent confidence.")
```

\pagebreak

## References